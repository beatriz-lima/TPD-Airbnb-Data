{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location Dimension ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this dimension we old information, of all possible locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![ETL Host](../images/location_schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The schema was in general maintained, with the addition of one new column, street_type. This column is extracted from official CTT documents, and so no processing is required. This column its a categorical value that describes the type of street for a certain location, some possible values are: Rua; Estrada Nacional; Praça; etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Hierarchies and data granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this dimension, all columns share relationships between them, since we are dealing with data that describes a physical location, mainly address information.\n",
    "\n",
    "It exists the following hierarchical chain: `Street` < `Parish` < `County`. These columns represent main components of the location's address, and so, obviously, share an hierarchical relationship, expressed in physical land boundaries.\n",
    "\n",
    "The columns `Street_Type`; `Zip_Code`; `Coastal_Area` share relationships with all other columns, and between them, although they are not, at least directly, hierarchical. They are extra attributes, that hold information on a specific location.\n",
    "\n",
    "We decided to use `Street` as our finner grain, since we want locations to generalized to more them one fact (one-to-many relationship). Any finner grain would compromise this objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ETL Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![ETL Host](../images/Pipeline_Location.png)\n",
    "\n",
    "In this ETL process, we add to take house specific address, and parse them to find the general street they describe. To this end we used a mixture of the open source NLP tools and case specific parsing functions. \n",
    "\n",
    "Unfortunately, there are no available tools for parsing Portuguese addresses specifically. So we coupled an Open Source library (`libpostal`), with some case specific logic, to solve issues we detected when trying to use the tool to parse Portuguese addresses. But since this field is open text, its impossible to deal with all possible address in this manner. To do this correctly in a production environment, an NLP model should be trained to identify generic streets in Portuguese addresses. We could even use `libpostal` as pretrained model, and them specify the model with Portuguese addresses specific training.\n",
    "\n",
    "Right now we are able to obtain a semi-optimal result for all locations, with some streets more generalized them others. In the mapping phase we address possible duplicates, by checking if any street can be considered a subset of another street, and if they share a similar zip code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Slow Changing Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Locations are not expected to change.** New locations can be added and Facts can map to different dimensions. But since we deal with general locations (street level), we don't expect them to change, and always want them to exist. Even if a certain location is not being used by any fact at a given time, it can always be mapped to a new fact. New facts will always try to map to existing locations, so existing locations will always be useful.\n",
    "\n",
    "Changes might be needed in very exceptional situations like, changes in: County/Parish name/boudaries; Street name; Portuguese Postal system. So mainly changes that might never happen.\n",
    "\n",
    "In light of this, we consider this to be a **slow changing dimension of type I**, where if changes are required, specific SQL would be written to perform bulk updates to the DBs. For example in case county A and county B become county C, we would need to update all locations to that are in counties A or B to county C. \n",
    "\n",
    "In this ETL we don't consider this changes, since they are too sporadic, and can differ a lot given their origin.\n",
    "\n",
    "We choose to model these changes as type I, because we don't see any major advantage of having a record of this changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part 0 - Listings|Location - EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Library and Function Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import Libs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load Functions\n",
    "\n",
    "def missing_data(df, columns):\n",
    "    df_graph = df[columns]\n",
    "    #missing data\n",
    "    total = df_graph.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (df_graph.isnull().sum()/df_graph.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    print(missing_data)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    ax = sns.barplot(x='index',y = 'Percent', data= missing_data.reset_index())\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "    return ax\n",
    "\n",
    "def feature_dist(df, column_name):\n",
    "    plt.figure(figsize=(15,4))\n",
    "    sns.distplot(df[column_name] , fit=norm)\n",
    "\n",
    "    # Get the fitted parameters used by the function\n",
    "    (mu, sigma) = norm.fit(df[column_name])\n",
    "    print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "                loc='best')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('{} distribution'.format(column_name))\n",
    "    plt.show()\n",
    "\n",
    "#IN BAR CHARTS, SET PERCENTAGES ABOVE EACH BAR\n",
    "def set_bar_percentage(ax, df):\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f\"{p.get_height() * 100 / df.shape[0]:.2f}%\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "          ha='center', va='center', rotation=0, xytext=(0, 10),\n",
    "          textcoords='offset points')\n",
    "        \n",
    "#IN BAR CHARTS, SET THE VALUE ABOVE EACH BAR\n",
    "def set_bar_amount(ax):\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.annotate(\"{0:.2f}%\".format(p.get_height()*100), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "            ha='center', va='center', rotation=0, xytext=(0, 10),\n",
    "            textcoords='offset points')\n",
    "        \n",
    "#Simple plot\n",
    "def simple_plot(df,column):\n",
    "    bx = sns.catplot(x=column,data=df, kind='count')\n",
    "    (bx.set_axis_labels(column, \"Count\")\n",
    "        .set_titles(\"{col_name} {col_var}\")\n",
    "        .despine(left=True))\n",
    "    \n",
    "def intTryParse(value):\n",
    "    try:\n",
    "        a = int(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load CSV files\n",
    "\n",
    "df_result = pd.read_csv(\"../data/listings_al.csv\", low_memory=False)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('dataset size: {}'.format(len(df_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'Endereco',\n",
    "    'CodigoPostal',\n",
    "    'FreguesiasCosteiras'\n",
    "    ]\n",
    "ax = missing_data(df_result,columns)\n",
    "set_bar_amount(ax)\n",
    "ax.set_xlabel('Columns')\n",
    "ax.set_ylabel('Missing data (%)')\n",
    "plt.ylim(0,1)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.set_title('Location dimension missing data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Street**\n",
    "\n",
    "Some repeated addresses. Reapeated 'Rua'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_result['Endereco'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Zip_code**\n",
    "\n",
    "- Some missing values - 21\n",
    "- Some repeated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(df_result['CodigoPostal'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Coastal Area**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(df_result['FreguesiasCosteiras'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part 1- Raw Data Processing\n",
    "\n",
    "**UNIX systems only**\n",
    "\n",
    "This part uses pypostal, which are python bindings for the libpostal C library.\n",
    "\n",
    "\"Libpostal is a C library for parsing/normalizing street addresses around the world using statistical NLP and open data. The goal of this project is to understand location-based strings in every language, everywhere.\"\n",
    "\n",
    "[Follow this instructions to install pypostal](https://github.com/openvenues/pypostal#installation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Library and Function Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import Libs\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "#from postal.parser import parse_address\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load Functions\n",
    "\n",
    "def street_parser(s):\n",
    "    \n",
    "    s_final = ''\n",
    "    \n",
    "    sch = s[:int(len(s)/2)]\n",
    "\n",
    "    first_digit = re.search(r\"\\d\", sch)\n",
    "    \n",
    "    try:\n",
    "        s = s[:int(first_digit.start() + len(s)/2)]\n",
    "        \n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    s = s.split(' ')\n",
    "    s = [x.replace(' ', '') for x in s if x != '']\n",
    "\n",
    "    for ss in s:\n",
    "        s_final += '{} '.format(ss)\n",
    "        \n",
    "    return s_final[:-1]\n",
    "\n",
    "def is_int(n):\n",
    "    \n",
    "    try:\n",
    "        int(n)\n",
    "        return True\n",
    "    \n",
    "    except ValueError:\n",
    "        return False\n",
    "        \n",
    "\n",
    "def get_street(st, street_str=''):\n",
    "    \n",
    "    forbiden = ['DENOMINADO', 'Nº', 'NºS', 'NÚMEROS', 'N.º']\n",
    "    \n",
    "    street = parse_address(st)[0][0]\n",
    "    \n",
    "    if len(street) <= len(st)/2:\n",
    "        \n",
    "        street = street_parser(st)\n",
    "    \n",
    "    street = street.split(' ')\n",
    "    \n",
    "    for i, s in enumerate(street):\n",
    "        if s not in street_str:\n",
    "            if s in forbiden or (i > 3 and any(is_int(si) for si in s)):\n",
    "                break\n",
    "\n",
    "            street_str += s + ' '\n",
    "            \n",
    "    street_str.translate(str.maketrans('', '', string.punctuation))\n",
    "            \n",
    "    return (street_str[:-1])[:100].upper()\n",
    "\n",
    "def validate_zip(sample, expr=r\"(\\b\\d{4}-\\d{3}\\b)\"):\n",
    "    \n",
    "    zipCode = re.compile(expr)\n",
    "\n",
    "    try:\n",
    "        if zipCode.match(sample):\n",
    "            return sample\n",
    "        else:\n",
    "            return None\n",
    "    except TypeError:\n",
    "        return None\n",
    "    \n",
    "def delete_null_rows(df,columns):\n",
    "    \"\"\"Removes all records with any empty cells from input DataFrame\"\"\"\n",
    "    processed_df = df[columns].copy()\n",
    "    total_rows = processed_df.shape[0]\n",
    "    delete_rows = []\n",
    "    \n",
    "    if processed_df.isnull().values.any(): # if there are any null values in DataFrame, process DataFrame\n",
    "        for index, row in enumerate(processed_df.itertuples(), start = 0):\n",
    "            if (pd.Series(row).isnull().values.any()): # if row has any null value\n",
    "                delete_rows.append(index) # add row index to delete list\n",
    "    \n",
    "    final_df = df.copy()\n",
    "    final_df.drop(df.index[delete_rows], inplace = True) # delete rows fr\n",
    "    final_df = final_df.reset_index().drop('index', axis = 1)\n",
    "    print(\"DataFrame contains {} rows. Deleted {} rows ({}% of total rows)\".format(\n",
    "        len(final_df), len(delete_rows), round(len(delete_rows)*100/total_rows, 2)))\n",
    "    return final_df\n",
    "\n",
    "def check_zeros(ext, lenght=3):\n",
    "    \n",
    "    ext = str(ext)\n",
    "    \n",
    "    if len(ext) != lenght:\n",
    "        \n",
    "        return '0' * (lenght - len(ext)) + str(ext)\n",
    "    else:\n",
    "        return ext\n",
    "    \n",
    "def coastal_area(boolean):\n",
    "    if boolean == 'Sim':\n",
    "        return 'Coastal Area'\n",
    "    elif boolean == 'Não':\n",
    "        return 'Not Coastal Area'\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def map_duplicates(streets, zip_codes, ids):\n",
    "\n",
    "    temps = []\n",
    "    dup_id = []\n",
    "    main_id = []\n",
    "\n",
    "    for i, entry in enumerate(ids):\n",
    "\n",
    "\n",
    "        flag = True\n",
    "        for temp in temps:\n",
    "\n",
    "            if (streets[i].issubset(temp[0]) or streets[i] == temp[0])  and zip_codes[i][:4] == temp[1][:4]:\n",
    "\n",
    "                dup_id.append(entry)\n",
    "                main_id.append(temp[2])\n",
    "\n",
    "                flag = False\n",
    "                break\n",
    "\n",
    "        if flag is True:\n",
    "\n",
    "            temps.append([streets[i], zip_codes[i], entry])\n",
    "            dup_id.append(entry)\n",
    "            main_id.append(entry)\n",
    "            \n",
    "    return(dup_id, main_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Import Data\n",
    "\n",
    "df = pd.read_csv(\"../data/listings_al.csv\", low_memory=False)\n",
    "df_cp = pd.read_csv(\"../data/codigos_postais.csv\", low_memory=False)\n",
    "df_c = pd.read_csv(\"../data/concelhos.csv\", low_memory=False)\n",
    "df_d = pd.read_csv(\"../data/distritos.csv\", low_memory=False)\n",
    "df_d = df_d[df_d['nome_distrito'] == 'Lisboa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Merge CTT Data\n",
    "\n",
    "df_cd = pd.merge(df_c, df_d, on='cod_distrito')\n",
    "df_ctt = pd.merge(df_cp, df_cd, on=['cod_concelho', 'cod_distrito'])\n",
    "\n",
    "df_ctt['cp'] = [str(df_ctt['num_cod_postal'][i]) + check_zeros(df_ctt['ext_cod_postal'][i]) \n",
    "                for i in range(len(df_ctt))]\n",
    "\n",
    "delete = ['cod_distrito', 'cod_concelho', 'cod_localidade', 'cod_arteria', 'prep1', 'titulo_arteria', \n",
    "          'prep2', 'local_arteria', 'troco', 'porta', 'cliente', 'desig_postal', 'nome_arteria', \n",
    "          'ext_cod_postal', 'num_cod_postal']\n",
    "df_ctt.drop(delete, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_ctt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Missing Data\n",
    "\n",
    "df_ctt.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Parse Zip Codes\n",
    "\n",
    "df['CodigoPostal'] = [validate_zip(sample) for sample in df['CodigoPostal']]\n",
    "\n",
    "# Delete Records with Invalid Zip Codes\n",
    "\n",
    "df = delete_null_rows(df, ['CodigoPostal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Prepare Final Merge\n",
    "        \n",
    "df['cp'] = [cp.replace('-', '')  for cp in df['CodigoPostal']]\n",
    "\n",
    "before = len(df)\n",
    "\n",
    "# Merge All Data\n",
    "\n",
    "df = pd.merge(df, df_ctt, on='cp', how='inner')\n",
    "df.drop_duplicates(subset=['id'], inplace=True)\n",
    "print('{} rows after merge: {} rows deleted'.format(len(df), before-len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some of the properties, are not in the Lisbon district so they were eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Parse Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Build Location DataFrame\n",
    "\n",
    "df_loc = pd.DataFrame()\n",
    "\n",
    "df_loc['location_id'] = [value for value in df['id']]\n",
    "df_loc['street'] = [get_street(value) for value in df['Endereco']]\n",
    "df_loc['street_type'] = [value[:40] for value in df['tipo_arteria']]\n",
    "df_loc['zip_code'] = [value for value in df['CodigoPostal']]\n",
    "df_loc['parish'] = [value[:40] for value in df['nome_localidade']]\n",
    "df_loc['county'] = [value[:40] for value in df['nome_concelho']]\n",
    "df_loc['coastal_area'] = [coastal_area(value) for value in df['FreguesiasCosteiras']]\n",
    "\n",
    "# Sorting values for FK mapping\n",
    "\n",
    "ind = df_loc.street.str.len().sort_values(ascending=False).index\n",
    "df_loc = df_loc.reindex(ind)\n",
    "df_loc.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_loc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Right now location as a one to one relationship with listings, since in reality they have a one to many relationship, this means we have duplicates. The last processing needed is to remove all duplicates, while mapping a fk to a respective listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Map Duplicates\n",
    "\n",
    "streets = [set(street.split(' ')) for street in df_loc['street']]\n",
    "zip_codes = df_loc['zip_code']\n",
    "ids = df_loc['location_id']\n",
    "\n",
    "(dup_id, main_id) = map_duplicates(streets, zip_codes, ids)\n",
    "\n",
    "# Remove Duplicates\n",
    "\n",
    "loc = df_loc[df_loc.location_id.isin(set(main_id))]\n",
    "loc.reset_index(drop=True, inplace=True)\n",
    "loc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Build index dataframe\n",
    "index = [i for i in loc.index]\n",
    "location_id = [location for location in loc['location_id']]\n",
    "columns = ['fk', 'main_id']\n",
    "\n",
    "ind = pd.DataFrame(data=[index, location_id]).T\n",
    "ind.columns = columns\n",
    "\n",
    "# Build FK dataframe\n",
    "\n",
    "columns = ['listings_id', 'main_id']\n",
    "fk_map = pd.DataFrame(data=[dup_id, main_id]).T\n",
    "fk_map.columns = columns\n",
    "\n",
    "# Merge dataframes\n",
    "\n",
    "fk_map = pd.merge(ind, fk_map, on='main_id', how='inner')\n",
    "fk_map.drop(['main_id'], axis=1, inplace=True)\n",
    "fk_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loc.drop(['location_id'], axis=1, inplace=True)\n",
    "loc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Last check for null values before exportation.\n",
    "\n",
    "print(loc.isnull().sum())\n",
    "print(fk_map.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Export datasets into csv\n",
    "\n",
    "loc.to_csv('../processed_dt/location.csv', index=False)\n",
    "fk_map.to_csv('../processed_dt/location_fk.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Injecting Database "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load Libs and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import Libs\n",
    "\n",
    "import pandas as pd\n",
    "import pandas.io.sql as sqlio\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load Functions\n",
    "\n",
    "#for that method to run, the dataframes must have the same columns, in the same order\n",
    "def get_data_to_insert(df1, df2, columns):\n",
    "    #if reverse is True:\n",
    "    return df1[~df1[columns].apply(tuple,1).isin(df2[columns].apply(tuple,1))]\n",
    "\n",
    "def run_sql_command(sql, conn):\n",
    "    \"\"\"Executes a single SQL statement from a string variable and the database credentials\"\"\"\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    cur.close()\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "def query_table(conn, table_name):\n",
    "    \"\"\"Returns DataFrame with queried database table\"\"\"\n",
    "    sql = \"select * from {};\".format(table_name)\n",
    "    #return dataframe\n",
    "    return sqlio.read_sql_query(sql, conn)\n",
    "\n",
    "def insert_data(df, table_name, conn):\n",
    "    \"\"\"Inserts selected data into dimension table in database\"\"\"\n",
    "    df_columns = list(df)\n",
    "    columns = \",\".join(df_columns)\n",
    "    values = \"VALUES({})\".format(\",\".join([\"%s\" for _ in df_columns])) \n",
    "    insert_stmt = \"INSERT INTO {} ({}) {}\".format(table_name,columns,values)\n",
    "    success = True\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        psycopg2.extras.execute_batch(cursor, insert_stmt, df.values)\n",
    "        conn.commit()\n",
    "        success = True\n",
    "    except psycopg2.DatabaseError as error:\n",
    "        success = False\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "    return success\n",
    "\n",
    "def re_map(main_df, diff_df, sql_df, fk_df):\n",
    "    \n",
    "    # Ensure Mapping index starts at N (where N is the number of locations already in the database)\n",
    "\n",
    "    try: \n",
    "        N = sql_df['location_id'].iloc[-1] + 1\n",
    "    except IndexError:\n",
    "        N = 1\n",
    "\n",
    "    fk_df['fk'] = [value + N for value in fk_df['fk']]\n",
    "\n",
    "    # Add Mapping to main dataframe\n",
    "\n",
    "    main_df['fk'] = set(fk_df['fk'])\n",
    "\n",
    "    # Get delete rows\n",
    "\n",
    "    df_rm = get_data_to_insert(main_df, diff_df, columns=['street', 'zip_code'])\n",
    "\n",
    "    # Add PK\n",
    "    df_pk = sql_df[['location_id', 'street', 'zip_code']]\n",
    "    df_rm = pd.merge(df_rm, df_pk, on=['street', 'zip_code'], how='left')\n",
    "\n",
    "    # Keep only necessary rows\n",
    "\n",
    "    df_rm = df_rm[['fk', 'location_id']]\n",
    "\n",
    "    # Merge mapping with location_id\n",
    "\n",
    "    temp_map = pd.merge(df_rm, fk_df, on=['fk'], how='right')\n",
    "\n",
    "    # Re-map\n",
    "\n",
    "    temp_map['fk'] = [temp_map['fk'][i] if math.isnan(value) else int(value) for i, value in enumerate(temp_map['location_id'])]\n",
    "    temp_map = temp_map.drop(['location_id'], axis=1)\n",
    "\n",
    "    # Re-index\n",
    "\n",
    "    count = N\n",
    "    fks = []\n",
    "    previous = dict()\n",
    "    for fk in temp_map['fk']:\n",
    "        if fk >= N:\n",
    "            if fk in previous.keys():\n",
    "                fks.append(previous[fk])\n",
    "            else:\n",
    "                previous[fk] = count\n",
    "                fks.append(count)\n",
    "                count += 1\n",
    "        else:\n",
    "            fks.append(fk)\n",
    "\n",
    "    temp_map['fk'] = fks\n",
    "    \n",
    "    return temp_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### DB Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from db_connection import dbconnection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>street</th>\n",
       "      <th>street_type</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>parish</th>\n",
       "      <th>county</th>\n",
       "      <th>coastal_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROTUNDA JOÃO PAULO II E AVENIDA REI HUMBERTO D...</td>\n",
       "      <td>Rua</td>\n",
       "      <td>2750-641</td>\n",
       "      <td>Cascais</td>\n",
       "      <td>Cascais</td>\n",
       "      <td>Coastal Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>URBANIZAÇÃO VALE DA AZENHA, RUA DOS DESCOBRIME...</td>\n",
       "      <td>Urbanização</td>\n",
       "      <td>2560-510</td>\n",
       "      <td>Santa Cruz</td>\n",
       "      <td>Torres Vedras</td>\n",
       "      <td>Coastal Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAVESSA CAMINHO DO PINHAL CONDOMÍNIO PARQUE A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2560-051</td>\n",
       "      <td>Praia do Navio</td>\n",
       "      <td>Torres Vedras</td>\n",
       "      <td>Coastal Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVENIDA COMISSÃO DE MELHORAMENTOS MIL NOVECENT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2705-001</td>\n",
       "      <td>Azoia</td>\n",
       "      <td>Sintra</td>\n",
       "      <td>Coastal Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RUA REI HUMBERTO II DE ITÁLIA CONDOMÍNIO CASAS...</td>\n",
       "      <td>Rua</td>\n",
       "      <td>2750-641</td>\n",
       "      <td>Cascais</td>\n",
       "      <td>Cascais</td>\n",
       "      <td>Coastal Area</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              street  street_type  zip_code  \\\n",
       "0  ROTUNDA JOÃO PAULO II E AVENIDA REI HUMBERTO D...          Rua  2750-641   \n",
       "1  URBANIZAÇÃO VALE DA AZENHA, RUA DOS DESCOBRIME...  Urbanização  2560-510   \n",
       "2  TRAVESSA CAMINHO DO PINHAL CONDOMÍNIO PARQUE A...          NaN  2560-051   \n",
       "3  AVENIDA COMISSÃO DE MELHORAMENTOS MIL NOVECENT...          NaN  2705-001   \n",
       "4  RUA REI HUMBERTO II DE ITÁLIA CONDOMÍNIO CASAS...          Rua  2750-641   \n",
       "\n",
       "           parish         county  coastal_area  \n",
       "0         Cascais        Cascais  Coastal Area  \n",
       "1      Santa Cruz  Torres Vedras  Coastal Area  \n",
       "2  Praia do Navio  Torres Vedras  Coastal Area  \n",
       "3           Azoia         Sintra  Coastal Area  \n",
       "4         Cascais        Cascais  Coastal Area  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Data\n",
    "df = pd.read_csv('../processed_dt/location.csv', low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create Table\n",
    "\n",
    "create_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Location (\n",
    "    LOCATION_ID SERIAL PRIMARY KEY NOT NULL,\n",
    "    STREET VARCHAR(100) NOT NULL ,\n",
    "    STREET_TYPE VARCHAR(40),\n",
    "    PARISH VARCHAR(40) NOT NULL,\n",
    "    COUNTY VARCHAR(40) NOT NULL,\n",
    "    ZIP_CODE VARCHAR(8) NOT NULL,\n",
    "    COASTAL_AREA VARCHAR(20) CHECK (COASTAL_AREA in ('Coastal Area', 'Not Coastal Area')),\n",
    "    UNIQUE (STREET, ZIP_CODE)\n",
    ");\n",
    "\"\"\"\n",
    "conn = psycopg2.connect(host = dbconnection.server_host,database = dbconnection.dbname, user = dbconnection.dbusername,password = dbconnection.dbpassword,sslmode=dbconnection.sslmode,gssencmode=dbconnection.gssencmode)\n",
    "run_sql_command(sql=create_table, conn=conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>street</th>\n",
       "      <th>c</th>\n",
       "      <th>parish</th>\n",
       "      <th>county</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>coastal_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [location_id, street, c, parish, county, zip_code, coastal_area]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query Database\n",
    "\n",
    "conn = psycopg2.connect(host = dbconnection.server_host,database = dbconnection.dbname, user = dbconnection.dbusername,password = dbconnection.dbpassword,sslmode=dbconnection.sslmode,gssencmode=dbconnection.gssencmode)\n",
    "df_sql = query_table(conn, 'location')\n",
    "conn.close()\n",
    "df_sql.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>street</th>\n",
       "      <th>street_type</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>parish</th>\n",
       "      <th>county</th>\n",
       "      <th>coastal_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROTUNDA JOÃO PAULO II E AVENIDA REI HUMBERTO D...</td>\n",
       "      <td>Rua</td>\n",
       "      <td>2750-641</td>\n",
       "      <td>Cascais</td>\n",
       "      <td>Cascais</td>\n",
       "      <td>Coastal Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>URBANIZAÇÃO VALE DA AZENHA, RUA DOS DESCOBRIME...</td>\n",
       "      <td>Urbanização</td>\n",
       "      <td>2560-510</td>\n",
       "      <td>Santa Cruz</td>\n",
       "      <td>Torres Vedras</td>\n",
       "      <td>Coastal Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAVESSA CAMINHO DO PINHAL CONDOMÍNIO PARQUE A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2560-051</td>\n",
       "      <td>Praia do Navio</td>\n",
       "      <td>Torres Vedras</td>\n",
       "      <td>Coastal Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVENIDA COMISSÃO DE MELHORAMENTOS MIL NOVECENT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2705-001</td>\n",
       "      <td>Azoia</td>\n",
       "      <td>Sintra</td>\n",
       "      <td>Coastal Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RUA REI HUMBERTO II DE ITÁLIA CONDOMÍNIO CASAS...</td>\n",
       "      <td>Rua</td>\n",
       "      <td>2750-641</td>\n",
       "      <td>Cascais</td>\n",
       "      <td>Cascais</td>\n",
       "      <td>Coastal Area</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              street  street_type  zip_code  \\\n",
       "0  ROTUNDA JOÃO PAULO II E AVENIDA REI HUMBERTO D...          Rua  2750-641   \n",
       "1  URBANIZAÇÃO VALE DA AZENHA, RUA DOS DESCOBRIME...  Urbanização  2560-510   \n",
       "2  TRAVESSA CAMINHO DO PINHAL CONDOMÍNIO PARQUE A...          NaN  2560-051   \n",
       "3  AVENIDA COMISSÃO DE MELHORAMENTOS MIL NOVECENT...          NaN  2705-001   \n",
       "4  RUA REI HUMBERTO II DE ITÁLIA CONDOMÍNIO CASAS...          Rua  2750-641   \n",
       "\n",
       "           parish         county  coastal_area  \n",
       "0         Cascais        Cascais  Coastal Area  \n",
       "1      Santa Cruz  Torres Vedras  Coastal Area  \n",
       "2  Praia do Navio  Torres Vedras  Coastal Area  \n",
       "3           Azoia         Sintra  Coastal Area  \n",
       "4         Cascais        Cascais  Coastal Area  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-Validate if data exists on DB\n",
    "\n",
    "df_etl = get_data_to_insert(df, df_sql, columns=['street', 'zip_code'])\n",
    "df_etl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3633"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_etl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Re-Map FK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fk</th>\n",
       "      <th>listings_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12194911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>40962519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39414401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>16883483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>12978864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fk  listings_id\n",
       "0   1     12194911\n",
       "1   2     40962519\n",
       "2   3     39414401\n",
       "3   4     16883483\n",
       "4   5     12978864"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Mapping\n",
    "\n",
    "fk_map = pd.read_csv('../processed_dt/location_fk.csv', low_memory=False)\n",
    "fk_map.sort_values(['fk'], inplace=True)\n",
    "fk_map.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At this stage we re-map fk's to assert the fk's of listings for which locations already existed on the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fk</th>\n",
       "      <th>listings_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12194911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>40962519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39414401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>16883483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>12978864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fk  listings_id\n",
       "0   1     12194911\n",
       "1   2     40962519\n",
       "2   3     39414401\n",
       "3   4     16883483\n",
       "4   5     12978864"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-map\n",
    "final_map = re_map(df, df_etl, df_sql, fk_map)\n",
    "final_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Export Mapping\n",
    "final_map.to_csv('../processed_dt/location_fk.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Insert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted succefully\n"
     ]
    }
   ],
   "source": [
    "if len(df_etl) > 0:\n",
    "    table_name = 'location'\n",
    "    conn = psycopg2.connect(host = dbconnection.server_host,database = dbconnection.dbname, user = dbconnection.dbusername,password = dbconnection.dbpassword,sslmode=dbconnection.sslmode,gssencmode=dbconnection.gssencmode)\n",
    "    success = insert_data(df_etl,table_name, conn)\n",
    "    conn.close()\n",
    "    if success == True: print('Data inserted succefully')\n",
    "else: print('No data to insert')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
