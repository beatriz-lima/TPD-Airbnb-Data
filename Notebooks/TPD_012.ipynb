{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tecnologias de Processamento de dados - 2019/2020\n",
    "\n",
    "## Phase II - Group 12\n",
    "\n",
    "\n",
    "|   Student      | Student ID |  Contribution in hours |\n",
    "|----------------|------------|----------------|\n",
    "| Beatriz Lima   |    49377   |   |\n",
    "| David Almeida  |    54120   |   |\n",
    "|João Castanheira|    55052   |   |\n",
    "| Pedro Cotovio  |    55053   |   |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "import psycopg2 as pg\n",
    "import psycopg2.extras\n",
    "import pandas.io.sql as sqlio\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main datasets used in this warehouse are:\n",
    "\n",
    "- http://insideairbnb.com/get-the-data.html for Lisbon, Portugal. - listings.csv\n",
    "- https://dadosabertos.turismodeportugal.pt/datasets/alojamento-local) - Alojamento_Local.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../data/Alojamento_Local.csv' does not exist: b'../data/Alojamento_Local.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-99ba6be78af2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlistings_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/airbnb/listings.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mal_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/Alojamento_Local.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_al\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mal_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_listings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistings_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../data/Alojamento_Local.csv' does not exist: b'../data/Alojamento_Local.csv'"
     ]
    }
   ],
   "source": [
    "listings_file_path = '../data/airbnb/listings.csv'\n",
    "al_file_path = '../data/Alojamento_Local.csv'\n",
    "df_al = pd.read_csv(al_file_path)\n",
    "df_listings = pd.read_csv(listings_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Merge _df_listings_ with _alojamento_local.csv_\n",
    "\n",
    "In order to enrich the main dataset, we can cross it with the dataset from Registo Nacional de Alojamento Local (RNAL) to obtain further information regarding each listing's property, as well as refine already available data, particularly in the case of location data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def intTryParse(value):\n",
    "    \"\"\"Tries to parse string to an integer\"\"\"\n",
    "    try:\n",
    "        a = int(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get only listings where \n",
    "df_listings_with_license = df_listings[(~df_listings['license'].isnull()) #'license' is not null\n",
    "                                        & (df_listings['license'] != 'Exempt')] # && != 'Exempt'\n",
    "\n",
    "# string replace\n",
    "df_listings_with_license['NrRNAL'] = [s.replace('/AL','').replace('.','') # remove '/AL' and '.' from code\n",
    "                                      for s in df_listings_with_license['license']]\n",
    "\n",
    "# get only records where license nr can be converted to int \n",
    "df_listings_with_license = df_listings_with_license[[intTryParse(s) # if code can be converted to int\n",
    "                                                     for s in df_listings_with_license['NrRNAL']]] # keep it\n",
    "\n",
    "# convert NrRNAL to int before merge the two dataframes\n",
    "df_listings_with_license['NrRNAL'] = df_listings_with_license['NrRNAL'].astype(np.int64) # convert code to int\n",
    "\n",
    "# inner join two dataframes\n",
    "df_listings_al = pd.merge(df_listings_with_license, df_al, how='inner', on='NrRNAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Save the intersection of the two files to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "listings_al_file_path = '../data/listings_al.csv'\n",
    "df_listings_al.to_csv(listings_al_file_path,index=False)\n",
    "print('Dataset size: {}'.format(len(df_listings_al)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dimensions and facts tables of the data warehouse\n",
    "\n",
    "+ Define and model them in SQL\n",
    "+ Identify hierarchies and fact granularity\n",
    "+ Create the dimensions and facts tables in the DBMS (postgreSQL)\n",
    "\n",
    "These steps are all described in the individual ETL notebooks for each dimension.\n",
    "\n",
    "As for each fact's granularity, a listings fact refers to each advertisement in the Airbnb website, which is defined to be owned by a certain host, posted on a certain date, with a property in a determined location, a review profile with scores and a defined price per night of stay. A bookings fact refers to a calendar day where a certain property has been booked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General schema\n",
    "\n",
    "![Star schema](images/Schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define an ETL workflow\n",
    "\n",
    "![ETL pipeline](images/ETL_overall.png)\n",
    "\n",
    "### 2.1. Dimensions\n",
    "\n",
    "+ Identify all data sources for all dimensions. Add URL links to all data that should be available. If not public data, point to dropbox files, Google drive, or whatever\n",
    "+ For each dimension show the code used for modeling, filtering and inserting data\n",
    "+ Describe the process for inserting facts data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ETL workflow is defined in separate notebooks for each dimension:\n",
    "\n",
    " - ETL_Property.ipynb\n",
    " - ETL_Host.ipynb\n",
    " - ETL_Review.ipynb\n",
    " - ETL_Date.ipynb\n",
    " - ETL_Location.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Processing and inserting facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_connection import dbconnection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listing_price(listing_id):\n",
    "    return int(df_listings_al[df_listings_al['id']==listing_id].price.values[0].strip().split('.')[0].replace(',','').replace('$',''))\n",
    "\n",
    "# function to query table and convert it to pandas dataframe\n",
    "def query_table(conn, table_name):\n",
    "    \"\"\"Returns DataFrame with queried database table\"\"\"\n",
    "    sql = \"select * from {};\".format(table_name)\n",
    "    #return dataframe\n",
    "    return sqlio.read_sql_query(sql, conn)\n",
    "\n",
    "# for this function to run, the dataframes must have the same columns, in the same order\n",
    "def get_data_to_insert(df_etl, df_sql,pk):\n",
    "    \"\"\"Returns data valid for insertion in dimension from a new ETL-processed DataFrame\"\"\"\n",
    "    if isinstance(pk, list): \n",
    "        df_insert = df_etl[~df_etl[pk].apply(tuple,1).isin(df_sql[pk].apply(tuple,1))]\n",
    "        df_insert = df_insert.drop_duplicates(subset=pk)\n",
    "    else:  \n",
    "        df_insert = df_etl[-df_etl[pk].astype(int).isin(df_sql[pk].astype(int))].dropna(how = 'all')\n",
    "        df_insert = df_insert.drop_duplicates(subset=[pk])\n",
    "    return df_insert\n",
    "\n",
    "# function for bulk insert\n",
    "def insert_data(df, table_name, conn):\n",
    "    \"\"\"Inserts selected data into dimension table in database\"\"\"\n",
    "    df_columns = list(df)\n",
    "    columns = \",\".join(df_columns)\n",
    "    values = \"VALUES({})\".format(\",\".join([\"%s\" for _ in df_columns])) \n",
    "    insert_stmt = \"INSERT INTO {} ({}) {}\".format(table_name,columns,values)\n",
    "    success = True\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        psycopg2.extras.execute_batch(cursor, insert_stmt, df.values)\n",
    "        conn.commit()\n",
    "        success = True\n",
    "    except pg.DatabaseError as error:\n",
    "        success = False\n",
    "        print('error:{}'.format(error))\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "    return success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. _'Listings'_ fact table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the mappings between each dimension and the listing fact table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_date_path = '../processed_dt/df_listings_date.csv'\n",
    "listings_host_path = '../processed_dt/df_listings_host.csv'\n",
    "listings_property_path = '../processed_dt/df_listings_property.csv'\n",
    "listings_review_path = '../processed_dt/df_listings_review.csv'\n",
    "listings_location_path = '../processed_dt/location_fk.csv'\n",
    "\n",
    "df_listings_date = pd.read_csv(listings_date_path)[['listing_id','date_id']]\n",
    "df_listings_host = pd.read_csv(listings_host_path)[['listing_id','host_id']]\n",
    "df_listings_property = pd.read_csv(listings_property_path).rename(columns={'ID':'listing_id','Property':'property_id'})[['listing_id','property_id']]\n",
    "df_listings_review = pd.read_csv(listings_review_path)[['listing_id','review_id']]\n",
    "df_listings_location = pd.read_csv(listings_location_path).rename(columns={'fk':'location_id','listings_id':'listing_id'})[['listing_id','location_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inner join all dataframes by 'listing_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inner join all dataframes, by listing_id\n",
    "dfs = [df_listings_date, df_listings_host, df_listings_property, df_listings_review, df_listings_location]\n",
    "df_listings_facts_etl = reduce(lambda  left,right: pd.merge(left,right,on=['listing_id'], how='inner'), dfs)\n",
    "#get the fact metric\n",
    "df_listings_facts_etl['price_per_night'] = [get_listing_price(i) for i in df_listings_facts_etl['listing_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove listings where price_per_night = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_facts_etl = df_listings_facts_etl[df_listings_facts_etl['price_per_night']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query listings table and convert it to dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>host_id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>property_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>price_per_night</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [listing_id, host_id, date_id, location_id, property_id, review_id, price_per_night]\n",
       "Index: []"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = psycopg2.connect(host = dbconnection.server_host,database = dbconnection.dbname, user = dbconnection.dbusername,password = dbconnection.dbpassword,sslmode=dbconnection.sslmode,gssencmode=dbconnection.gssencmode)\n",
    "df_listings_facts_sql = query_table(conn, 'listings')\n",
    "conn.close()\n",
    "df_listings_facts_sql.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get only new listings that are not in the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>host_id</th>\n",
       "      <th>property_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>price_per_night</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [listing_id, date_id, host_id, property_id, review_id, location_id, price_per_night]\n",
       "Index: []"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_listings_insert = get_data_to_insert(df_listings_facts_etl,df_listings_facts_sql,'listing_id')\n",
    "df_listings_insert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert listings into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = dbconnection.server_host,database = dbconnection.dbname, user = dbconnection.dbusername,password = dbconnection.dbpassword,sslmode=dbconnection.sslmode,gssencmode=dbconnection.gssencmode)\n",
    "df_date_sql = query_table(conn, 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/50626058/psycopg2-cant-adapt-type-numpy-int64\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "psycopg2.extensions.register_adapter(np.int64, psycopg2._psycopg.AsIs)\n",
    "\n",
    "if len(df_listings_insert) > 0:\n",
    "    table_name = 'listings'\n",
    "    conn = psycopg2.connect(host = dbconnection.server_host,database = dbconnection.dbname, user = dbconnection.dbusername,password = dbconnection.dbpassword,sslmode=dbconnection.sslmode,gssencmode=dbconnection.gssencmode)\n",
    "    success = insert_data(df_listings_insert,table_name, conn)\n",
    "    conn.close()\n",
    "    if success == True: print('Data inserted successfully')\n",
    "else: print('No data to insert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data in listings fact table and save it in `df_listings_facts_sql` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = dbconnection.server_host,database = dbconnection.dbname, user = dbconnection.dbusername,password = dbconnection.dbpassword,sslmode=dbconnection.sslmode,gssencmode=dbconnection.gssencmode)\n",
    "df_listings_facts_sql = query_table(conn, 'listings')\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. _'Bookings'_ facts table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will load the Bookings fact table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_pk(date):\n",
    "    \"\"\"Builds date primary key\"\"\"\n",
    "    return int(date.strftime('%d%m%Y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from calendar.csv, which contains the data to insert into the Bookings fact table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9125846, 7)\n"
     ]
    }
   ],
   "source": [
    "df_calendar = pd.read_csv('../data/airbnb/calendar.csv')\n",
    "print(df_calendar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "      <th>adjusted_price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>maximum_nights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41791859</td>\n",
       "      <td>2020-01-28</td>\n",
       "      <td>t</td>\n",
       "      <td>$120.00</td>\n",
       "      <td>$120.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41791859</td>\n",
       "      <td>2020-01-29</td>\n",
       "      <td>t</td>\n",
       "      <td>$120.00</td>\n",
       "      <td>$120.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41791859</td>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>t</td>\n",
       "      <td>$120.00</td>\n",
       "      <td>$120.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41791859</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>t</td>\n",
       "      <td>$120.00</td>\n",
       "      <td>$120.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41791859</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>t</td>\n",
       "      <td>$120.00</td>\n",
       "      <td>$120.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1125.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id        date available    price adjusted_price  minimum_nights  \\\n",
       "0    41791859  2020-01-28         t  $120.00        $120.00             7.0   \n",
       "1    41791859  2020-01-29         t  $120.00        $120.00             7.0   \n",
       "2    41791859  2020-01-30         t  $120.00        $120.00             7.0   \n",
       "3    41791859  2020-01-31         t  $120.00        $120.00             7.0   \n",
       "4    41791859  2020-02-01         t  $120.00        $120.00             7.0   \n",
       "\n",
       "   maximum_nights  \n",
       "0          1125.0  \n",
       "1          1125.0  \n",
       "2          1125.0  \n",
       "3          1125.0  \n",
       "4          1125.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_calendar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calendar file has more than 9M records, around the number of listings * 365 days per year. For the purpose of this project, we will just read the first k rows of the calender file.\n",
    "\n",
    "We will retrieve only the records where available = 'f', which are the records that correspond to future bookings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 3)\n"
     ]
    }
   ],
   "source": [
    "#read just the first k items\n",
    "k = 100000\n",
    "df_bookings_etl = df_calendar[df_calendar['available'] == 'f'].iloc[:k][['listing_id','date','price']]\n",
    "\n",
    "#create columns with the date primary key\n",
    "df_bookings_etl['date_id'] = [date_pk(datetime.strptime(d, \"%Y-%m-%d\")) for d in df_bookings_etl['date']]\n",
    "\n",
    "#remove date column\n",
    "df_bookings_etl = df_bookings_etl.drop(['date'], axis=1)\n",
    "\n",
    "#rename columns price\n",
    "df_bookings_etl = df_bookings_etl.rename(columns={'price':'price_per_night'})\n",
    "\n",
    "#format column to int\n",
    "df_bookings_etl['price_per_night'] = [int(i.strip().split('.')[0].replace(',','').replace('$','')) for i in df_bookings_etl['price_per_night']]\n",
    "\n",
    "#drop duplicates if exists\n",
    "df_bookings_etl = df_bookings_etl.drop_duplicates(subset=['listing_id','date_id'])\n",
    "\n",
    "print(df_bookings_etl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>price_per_night</th>\n",
       "      <th>date_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41791859</td>\n",
       "      <td>120</td>\n",
       "      <td>28012020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41791859</td>\n",
       "      <td>120</td>\n",
       "      <td>29012020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41791859</td>\n",
       "      <td>120</td>\n",
       "      <td>30012020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41791859</td>\n",
       "      <td>120</td>\n",
       "      <td>31012020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41791859</td>\n",
       "      <td>120</td>\n",
       "      <td>1022020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id  price_per_night   date_id\n",
       "0    41791859              120  28012020\n",
       "1    41791859              120  29012020\n",
       "2    41791859              120  30012020\n",
       "3    41791859              120  31012020\n",
       "4    41791859              120   1022020"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bookings_etl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets merge the bookings data that we selected (the first 100k records) with the listings available in the database. With that we ensure that we are only inserting booking facts that contain some value for each foreign key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're doing this because during the dimensions' ETL process we do not consider some records that have missing values, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57566, 5)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bookings_etl = pd.merge(df_listings_facts_sql,df_bookings_etl,how='inner',on='listing_id')[['property_id','date_id_y','host_id','location_id','price_per_night_y']]\n",
    "df_bookings_etl = df_bookings_etl.rename(columns = {'price_per_night_y':'price_per_night'})\n",
    "df_bookings_etl = df_bookings_etl.rename(columns = {'date_id_y':'date_id'})\n",
    "df_bookings_etl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Bookings fact table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>property_id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>host_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>price_per_night</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [property_id, date_id, host_id, location_id, price_per_night]\n",
       "Index: []"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = psycopg2.connect(host = dbconnection.server_host,database = dbconnection.dbname, user = dbconnection.dbusername,password = dbconnection.dbpassword,sslmode=dbconnection.sslmode,gssencmode=dbconnection.gssencmode)\n",
    "df_bookings_sql = query_table(conn, 'booking')\n",
    "conn.close()\n",
    "df_bookings_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrive the data needed to insert only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>property_id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>host_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>price_per_night</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>28012020</td>\n",
       "      <td>107347</td>\n",
       "      <td>1194</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29012020</td>\n",
       "      <td>107347</td>\n",
       "      <td>1194</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>30012020</td>\n",
       "      <td>107347</td>\n",
       "      <td>1194</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>31012020</td>\n",
       "      <td>107347</td>\n",
       "      <td>1194</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1022020</td>\n",
       "      <td>107347</td>\n",
       "      <td>1194</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57561</th>\n",
       "      <td>112</td>\n",
       "      <td>22012021</td>\n",
       "      <td>7665008</td>\n",
       "      <td>743</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57562</th>\n",
       "      <td>112</td>\n",
       "      <td>23012021</td>\n",
       "      <td>7665008</td>\n",
       "      <td>743</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57563</th>\n",
       "      <td>112</td>\n",
       "      <td>24012021</td>\n",
       "      <td>7665008</td>\n",
       "      <td>743</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57564</th>\n",
       "      <td>112</td>\n",
       "      <td>25012021</td>\n",
       "      <td>7665008</td>\n",
       "      <td>743</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57565</th>\n",
       "      <td>112</td>\n",
       "      <td>26012021</td>\n",
       "      <td>7665008</td>\n",
       "      <td>743</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20483 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       property_id   date_id  host_id  location_id  price_per_night\n",
       "0                1  28012020   107347         1194               50\n",
       "1                1  29012020   107347         1194               50\n",
       "2                1  30012020   107347         1194               50\n",
       "3                1  31012020   107347         1194               50\n",
       "4                1   1022020   107347         1194               50\n",
       "...            ...       ...      ...          ...              ...\n",
       "57561          112  22012021  7665008          743               89\n",
       "57562          112  23012021  7665008          743               89\n",
       "57563          112  24012021  7665008          743               89\n",
       "57564          112  25012021  7665008          743               87\n",
       "57565          112  26012021  7665008          743               87\n",
       "\n",
       "[20483 rows x 5 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bookings_insert = get_data_to_insert(df_bookings_etl,df_bookings_sql,['property_id','date_id'])\n",
    "df_bookings_insert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert Bookings data into database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully\n"
     ]
    }
   ],
   "source": [
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "psycopg2.extensions.register_adapter(np.int64, psycopg2._psycopg.AsIs)\n",
    "\n",
    "if len(df_bookingsavailability_insert) > 0:\n",
    "    table_name = 'booking'\n",
    "    conn = psycopg2.connect(host = dbconnection.server_host,database = dbconnection.dbname, user = dbconnection.dbusername,password = dbconnection.dbpassword,sslmode=dbconnection.sslmode,gssencmode=dbconnection.gssencmode)\n",
    "    success = insert_data(df_bookings_insert,table_name, conn)\n",
    "    conn.close()\n",
    "    if success == True: print('Data inserted successfully')\n",
    "else: print('No data to insert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Critical assessment of the work\n",
    "+ Describe potential issues with the ETL procedure used\n",
    "+ Compare your schema to the one previously defined in phase I\n",
    "+ Discuss the issues for updating the data warehouse with novel data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential problem with this ETL is that the dimensions end up storing additional information. In each dimension, some records were discarded, given the ammount of missing data. The merging was only done in later stages of the processing, after dimensions' data were ready for loading. Thus, we end up keeping only the facts containing foreign keys for all dimensions, meaning that some records in the dimensions are actually never used.\n",
    "\n",
    "Nevertheless, these 'factless' dimension records don't actually take up much space, comparing with the overall size of the data warehouse, since the majority of space is taken by the facts table.\n",
    "\n",
    "Critical assessments of ETL for each dimension are explained in each dimension's notebook, including the issues for updating the data warehouse with novel data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
